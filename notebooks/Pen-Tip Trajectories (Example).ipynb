{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducible randomness\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pen-Tip Trajectories (Example)\n",
    "\n",
    "This example notebook aims to showcase some of the preprocessing methods and classification algorithms offered by Sequentia on the [Character Trajectories Data Set](https://archive.ics.uci.edu/ml/datasets/Character+Trajectories). This dataset consists of pen-tip trajectories generated by writing English letters on a [WACOM tablet](https://www.wacom.com/en-us). \n",
    "\n",
    "The **$x$-velocity**, **$y$-velocity** and **pen-tip force** were recorded.\n",
    "\n",
    "Some more specific details about the dataset:\n",
    "\n",
    "- The data consists of 2858 character samples\n",
    "- The data has been numerically differentiated and Gaussian smoothed\n",
    "- Only characters with a single 'PEN-DOWN' segment were considered (these characters are shown later)\n",
    "- Characters have been shifted so that their velocity profiles best match the mean of the set\n",
    "\n",
    "Each character sample is a 3-dimensional pen tip velocity (and force) trajectory. This is contained in matrix format, with 3 rows and $T$ columns where $T$ is the length of the character sample, which represents our observation sequence.\n",
    "\n",
    "---\n",
    "\n",
    "- [Dynamic Time Warping $k$-NN](#Dynamic-Time-Warping-%24k%24-NN)\n",
    "- [Hidden Markov Models](#Hidden-Markov-Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the dataset and extract the samples and labels, then convert them so that they are in a format compatible with Sequentia (see the <a href=\"https://nbviewer.jupyter.org/github/eonu/sequentia/blob/master/notebooks/1%20-%20Input%20Format%20%28Tutorial%29.ipynb\"><em>Input Format</em></a> notebook for more information): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/character-trajectories/mixoutALL_shifted.mat'\n",
    "\n",
    "try:\n",
    "    path = os.path.join(os.getcwd(), 'temp.mat')\n",
    "    print('Downloading dataset from {} ...'.format(url))\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    raise\n",
    "else:\n",
    "    with open(path, 'wb') as file:\n",
    "        print('Temporarily writing data to file ...')\n",
    "        file.write(response.content)\n",
    "        print('Loading data into numpy.ndarray ...')\n",
    "        data = loadmat(path)\n",
    "        print('Done!')\n",
    "finally:\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trajectories\n",
    "# NOTE: Transpose from 3xT to Tx3\n",
    "X = [x.T for x in data['mixout'][0]]\n",
    "print('Number of trajectories: {}'.format(len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only lowercase characters with a single pen-down segment were considered in this dataset. In total, there were 20 of these characters as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the set of unique labels and report the number of labels\n",
    "labels = [label[0] for label in data['consts'][0][0][3][0]]\n",
    "n_labels = len(labels)\n",
    "print('Labels: {}'.format(str(labels)))\n",
    "print('Number of labels: {}'.format(n_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distribution of observation sequence lengths\n",
    "plt.title('Histogram of observation sequence lengths')\n",
    "plt.xlabel('Number of time frames')\n",
    "plt.ylabel('Count')\n",
    "plt.hist([len(x) for x in X], bins=n_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation sequences in this dataset are padded so that all sequences in one class have the same duration. This can be undone by trimming zeros.\n",
    "\n",
    "Additionally, the sample rate of each trajectory recording was 200hzâ€“meaning that in every second, 200 pen-tip trajectories were recorded!\n",
    "\n",
    "Although keeping all of these frames/data-points might result in a more accurate classifier, it also significantly increases the time required for training or prediction. This is especially the case for $k$-NN, since it is a non-parametric classifier that requires going through each training example during prediction time.\n",
    "\n",
    "---\n",
    "\n",
    "To fix the problems of zero-padding and the excessive frame count, we can use the `Preprocess` class to perform zero-trimming and downsampling transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequentia.preprocessing import *\n",
    "\n",
    "pre = Preprocess([\n",
    "    # Trim zero-observations\n",
    "    TrimZeros(), \n",
    "    # Downsample with averaging and a downsample factor of n=15\n",
    "    Downsample(factor=15, method='mean')\n",
    "])\n",
    "\n",
    "# Display a summary of the preprocessing transformations\n",
    "pre.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an example trajectory for visualization\n",
    "x = X[0]\n",
    "\n",
    "# Downsample the example trajectory, using a downsample factor of n=10\n",
    "x_pre = pre.transform(x)\n",
    "\n",
    "# Create the plot to visualize the downsampling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(x)\n",
    "ax1.set_title('Original velocity and force pen-tip trajectory sample')\n",
    "ax1.legend(labels=['$x$ velocity', '$y$ velocity', 'pen-tip force'])\n",
    "ax2.plot(x_pre)\n",
    "ax2.set_title('Transformed velocity and force pen-tip trajectory sample')\n",
    "ax2.legend(labels=['$x$ velocity', '$y$ velocity', 'pen-tip force'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how zero-trimming affects the dataset, we will plot a histogram of the zero-trimmed observation sequence lengths. \n",
    "\n",
    "We can then apply this transformation to the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distribution of trimmed observation sequence lengths\n",
    "plt.title('Histogram of observation sequence lengths')\n",
    "plt.xlabel('Number of time frames')\n",
    "plt.ylabel('Count')\n",
    "plt.hist([len(x) for x in TrimZeros()(X)], bins=n_labels)\n",
    "plt.show()\n",
    "\n",
    "# Transform the entire dataset\n",
    "X = pre.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels\n",
    "y = [labels[idx - 1] for idx in data['consts'][0][0][4][0]]\n",
    "\n",
    "# Plot a histogram of the labels for each class\n",
    "plt.title('Histogram of the dataset label counts')\n",
    "plt.xlabel('Label (character)')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(y, bins=n_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split the dataset into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rng, shuffle=True)\n",
    "print('Training set size: {}'.format(len(X_train)))\n",
    "print('Test set size: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for displaying results (accuracy and confusion matrix)\n",
    "def show_results(acc, cm, dataset):\n",
    "    df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.title('Confusion matrix for {} set predictions'.format(dataset), fontsize=14)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    # Fix for matplotlib bug that cuts off top/bottom of seaborn visualizations\n",
    "    b, t = plt.ylim()\n",
    "    plt.ylim(b + 0.5, t - 0.5)\n",
    "    plt.show()\n",
    "    print('Accuracy: {:.2f}%'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping $k$-NN\n",
    "\n",
    "The $k$-Nearest Neighbor ($k$-NN) classifier is a conceptually simple machine learning algorithm that is also easy to implement. As a result, it is often used as a baseline, despite often being able to perform much better than more complex algorithms.\n",
    "\n",
    "However, applying $k$-NN to isolated observation sequences is not so straightforward since different observation sequences may have different durations, making it difficult to come up with a distance measure that can be used to compare the two sequences. \n",
    "\n",
    "One such appropriate distance measure is [Dynamic Time Warping](https://en.wikipedia.org/wiki/Dynamic_time_warping).\n",
    "When combined with $k$-NN, this makes a very powerful sequence classifier.\n",
    "\n",
    "---\n",
    "\n",
    "Importing, creating and fitting the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequentia.classifiers import KNNClassifier\n",
    "\n",
    "# Create and fit a kNN classifier using the single nearest neighbor and fast C compiled DTW functions\n",
    "clf = KNNClassifier(k=1, classes=labels, use_c=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict single or multiple examples, we can use the `predict` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the first test example\n",
    "clf.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predict the first 50 test examples\n",
    "predictions = clf.predict(X_test[:50])\n",
    "print(*predictions, sep=' ', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be sped up a bit by using multiple jobs, as specified by `n_jobs`. By default this is set to 1. A setting of -1 will use all available cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NOTE: Progress bars for predict() and evaluate() are only displayed in the console if multiple jobs are used\n",
    "predictions = clf.predict(X_test[:50], n_jobs=-1)\n",
    "print(*predictions, sep=' ', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the model's accuracy and confusion matrix on some data, we can use the `evaluate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "acc, cm = clf.evaluate(X_test, y_test, labels=labels, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(acc, cm, dataset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Dynamic Time Warping $k$-NN classification often works with near perfect performance, but suffers due to the fact that $k$-NN is a non-parametric machine learning algorithm. \n",
    "\n",
    "This means that we have to look through every training example when we make a single prediction. Even with FastDTW, downsampling and multi-processing, the example classification on the test set consisting of 572 examples **took over 10 minutes**!\n",
    "\n",
    "---\n",
    "\n",
    "As a result, parametric methods such as Hidden Markov Models are often more feasible to useâ€“but also generally perform worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "A group of HMMs can be a good classifier for isolated sequences. The main idea behind using HMMs for classification is as follows:\n",
    "\n",
    "1. Create $N$ HMMs $\\lambda_1,\\lambda_2,\\ldots,\\lambda_C$, each representing a different class (character in this case).\n",
    "2. Fit each of these HMMs only using the training examples labeled with the class that the HMM represents. _The Baum-Welch algorithm is used for training here_.\n",
    "3. For a new observation sequence $O$, calculate the likelihood of each HMM generating $O$â€“that is, calculate $\\mathbb{P}(O|\\lambda_c) \\quad \\forall c\\in\\{1, 2, \\ldots, C\\}$. _This is done using the Forward algorithm_.\n",
    "4. Then $O$ is then classified as the class corresponding to the HMM that was most likely to generate $O$, giving a classification rule of: \n",
    "\n",
    "$$c^*=\\mathop{\\arg\\max}_{c\\in\\{1,2,\\ldots,C\\}}\\mathbb{P}(O|\\lambda_c)$$\n",
    "\n",
    "**Note**: In order to account for some classes naturally occurring more frequently than others, we can instead introduce a prior by using the Maximum A Posterior (MAP) classification rule:\n",
    "\n",
    "$$c^*=\\mathop{\\arg\\max}_{c\\in\\{1,2,\\ldots,C\\}}\\mathbb{P}(O|\\lambda_c)\\mathbb{P}(\\lambda_c)$$\n",
    "\n",
    "---\n",
    "\n",
    "Creating the individual `GMMHMM` objects and fitting each one on the training examples corresponding to the label (character) that it represents:\n",
    "\n",
    "**Note**: Here we naively set the number of states for all HMMs to 10. In reality, you will probably want to have different numbers of states for HMMs that represent more complex or more simple characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequentia.classifiers import GMMHMM, HMMClassifier\n",
    "\n",
    "hmms = []\n",
    "for label in tqdm(labels, desc='Training HMMs'):\n",
    "    hmm = GMMHMM(label=label, n_states=10, n_components=5, topology='linear', random_state=rng)\n",
    "    hmm.set_random_initial()\n",
    "    hmm.set_random_transitions()\n",
    "    hmm.fit([X_train[i] for i, y_i in enumerate(y_train) if y_i == label])\n",
    "    hmms.append(hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `HMMClassifier` object collects each of the individual `GMMHMM` objects in order to create the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HMMClassifier()\n",
    "clf.fit(hmms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, cm = clf.evaluate(X_test, y_test)\n",
    "show_results(acc, cm, dataset='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
