{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b3f6b0",
   "metadata": {},
   "source": [
    "# Using the `KNNClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7110ac7",
   "metadata": {},
   "source": [
    "This notebook aims to showcase how Sequentia's implementation of a $k$-NN classifier based on the dynamic time warping distance measure can be used in a typical sequence classisfication task. \n",
    "\n",
    "In particular, we look at the [Free Spoken Digit Dataset](https://github.com/Jakobovski/free-spoken-digit-dataset), which is an open dataset consisting of audio recordings of spoken digits from 0 to 9. We are interested in the task of determining which digit was spoken based on the audio recordings. Since audio signals are sequential data, we can consider this as a sequence classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e2fd96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not find a valid installation of 'torchaudio' (>=0.8), which TorchFSDD depends on.\nVisit https://github.com/pytorch/audio for more instructions on installing this package.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/sequentia/lib/python3.8/site-packages/torchfsdd/__init__.py:29\u001b[0m, in \u001b[0;36mcheck_package\u001b[0;34m(pkg, min_version, url)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/sequentia/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchaudio'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchfsdd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchFSDDGenerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msequentia\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compose, Standardize, Transform\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msequentia\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNNClassifier\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/sequentia/lib/python3.8/site-packages/torchfsdd/__init__.py:43\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Check that the minimum dependency versions are installed\u001b[39;00m\n\u001b[1;32m     42\u001b[0m check_package(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, MIN_TORCH_VERSION, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mcheck_package\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorchaudio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMIN_TORCHAUDIO_VERSION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://github.com/pytorch/audio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Import classes from the package\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchFSDD, TorchFSDDGenerator\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/sequentia/lib/python3.8/site-packages/torchfsdd/__init__.py:33\u001b[0m, in \u001b[0;36mcheck_package\u001b[0;34m(pkg, min_version, url)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find a valid installation of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{pkg}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (>=\u001b[39m\u001b[38;5;132;01m{min_version}\u001b[39;00m\u001b[38;5;124m), which TorchFSDD depends on.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit \u001b[39m\u001b[38;5;132;01m{url}\u001b[39;00m\u001b[38;5;124m for more instructions on installing this package.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(pkg\u001b[38;5;241m=\u001b[39mpkg, url\u001b[38;5;241m=\u001b[39murl, min_version\u001b[38;5;241m=\u001b[39mmin_version)\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(msg)\n\u001b[1;32m     35\u001b[0m installed_version \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mversion(pkg)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(installed_version) \u001b[38;5;241m<\u001b[39m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(min_version):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Could not find a valid installation of 'torchaudio' (>=0.8), which TorchFSDD depends on.\nVisit https://github.com/pytorch/audio for more instructions on installing this package."
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from torchfsdd import TorchFSDDGenerator\n",
    "\n",
    "from sequentia.preprocessing import Compose, Standardize, Transform\n",
    "from sequentia.classifiers import KNNClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt, numpy as np, pandas as pd, seaborn as sns\n",
    "\n",
    "# Set seed for reproducible randomness\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ce670",
   "metadata": {},
   "source": [
    "To get started, we first decide to use [Mel-Frequency Cepstrum Coefficients (MFCCs)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), which are a common way to represent audio for machine learning. \n",
    "\n",
    "While there are methods that work directly with raw signals, the most common approach is to use transformed features such as MFCCs.\n",
    "\n",
    "In short, MFCCs are used to characterize a particular window (usually 20-40ms) of samples of the original signal, by applying signal processing techniques and transformations to obtain a number of representative coefficients. The signal is divided into windows (which may be overlapping), and MFCCs are computed for each one of these frames.\n",
    "\n",
    "Below, we specify 5 MFCCs along with a number of other configurations for the MFCCs.\n",
    "\n",
    "**Note**: 5 MFCCs is far less than what would be used in typical machine learning tasks, but we use a small number just for demonstration purposes.<br/>However, we later see that the classifier takes quite long, even with just 5 MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MFCC configuration\n",
    "sr = 8000\n",
    "n_mfcc = 5\n",
    "spec_kwargs = {'n_fft': 1024, 'hop_length': 256, 'power': 2}\n",
    "\n",
    "# Set number of classes\n",
    "n_digits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad0e09",
   "metadata": {},
   "source": [
    "The below code uses the [`torchfsdd`](https://github.com/eonu/torch-fsdd) package to load a local copy of the FSDD dataset into training and test set splits (80% and 20%, respectively).\n",
    "\n",
    "Unlike typical batch loading used with PyTorch, this classifier requires all data to be loaded into memory at once, which is why we use `load_all=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a generator for a local version of FSDD, loading all recordings into memory\n",
    "fsdd = TorchFSDDGenerator(version='local', path='recordings', load_all=True)\n",
    "\n",
    "# Create two Torch datasets for a train-test split from the generator\n",
    "train_set, test_set = fsdd.train_test_split(test_size=0.2)\n",
    "print('Training set size: {}'.format(len(train_set)))\n",
    "print('Test set size: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970906ae",
   "metadata": {},
   "source": [
    "Since `torchfsdd` is intended for creating PyTorch dataset splits, we have to convert these into Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Torch tensors to Numpy arrays\n",
    "X_train = [x.detach().cpu().numpy().flatten() for x in train_set.recordings]\n",
    "X_test = [x.detach().cpu().numpy().flatten() for x in test_set.recordings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822b27f",
   "metadata": {},
   "source": [
    "Next, we create a transformation that generates MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC(Transform):\n",
    "    def __init__(self, sr, n_mfcc, spec_kwargs):\n",
    "        super().__init__()\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.spec_kwargs = spec_kwargs\n",
    "        \n",
    "    def transform(self, x):\n",
    "        # Flatten to 1D Numpy array\n",
    "        x = x.flatten()\n",
    "        # Generate MFCCs\n",
    "        x = librosa.feature.mfcc(x, sr=self.sr, n_mfcc=self.n_mfcc+1, **self.spec_kwargs)\n",
    "        # Remove the first MFCC as it is a constant offset\n",
    "        x = x[1:]\n",
    "        # Transpose to TxD\n",
    "        return x.T\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Generate {} MFCCs with sample rate {}'.format(self.n_mfcc, self.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae39be",
   "metadata": {},
   "source": [
    "We now define a transformation pipeline that generates and standardizes MFCCs using the above class and the `Standardize` transformation from Sequentia. This sequence of transformations is packaged into a `Compose` class, which acts in a similar way as [`torchvision.transforms.Compose`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Compose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "transforms = Compose([\n",
    "    MFCC(sr, n_mfcc, spec_kwargs),\n",
    "    Standardize()\n",
    "])\n",
    "\n",
    "transforms.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1da5b",
   "metadata": {},
   "source": [
    "We apply the same transformations to both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_train = transforms(X_train)\n",
    "mfcc_test = transforms(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b992d7",
   "metadata": {},
   "source": [
    "The figure below shows the difference between the raw audio signal of the pronunciation of a digit 3, and the MFCC representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6343c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc, digit = mfcc_train[0], train_set.labels[0]\n",
    "audio, _ = librosa.load(train_set.files[0], sr=sr)\n",
    "time = np.linspace(0, len(audio) / sr, num=len(audio))\n",
    "\n",
    "# Raw audio\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 2.5))\n",
    "axs[0].set(title='Raw audio signal for sample digit {}'.format(digit), xlabel='Time (s)', ylabel='Amplitude')\n",
    "axs[0].plot(time, audio)\n",
    "\n",
    "# MFCCs\n",
    "axs[1].imshow(np.swapaxes(mfcc, 0, 1), interpolation='nearest', cmap='viridis', origin='lower', aspect='auto')\n",
    "axs[1].set(title='MFCCs for sample digit {}'.format(digit), xlabel='Frame', ylabel='MFCC',\n",
    "    xticks=np.arange(len(mfcc)), xticklabels=np.arange(1, len(mfcc) + 1),\n",
    "    yticks=np.arange(n_mfcc), yticklabels=np.arange(1, n_mfcc + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7743c",
   "metadata": {},
   "source": [
    "Now we create and 'fit' our $k$-NN classifier on the training data. We demonstrate the use of a custom distance weighting function, a constrained warping window, along with independent warping and fast C compiled functions provided by [`dtaidistance`](https://github.com/wannesm/dtaidistance) to speed up the dynamic time warping calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a kNN classifier using the single nearest neighbor\n",
    "clf = KNNClassifier(k=1, classes=range(10), \n",
    "    weighting=(lambda x: np.exp(-x)), window=0.2, \n",
    "    use_c=True, independent=True)\n",
    "clf.fit(mfcc_train, train_set.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfca05",
   "metadata": {},
   "source": [
    "Evaluating the model on the test set, we can see that even with the above specifications and using multiple jobs, $k$-NN still suffers from being a much slower classifier compared to others, especially when the training set is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc, cm = clf.evaluate(mfcc_test, test_set.labels, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e144f9",
   "metadata": {},
   "source": [
    "Though we still end up with very good results; even when just using a single neighbor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display accuracy and confusion matrix\n",
    "labels = range(10)\n",
    "df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(df, annot=True, cbar=False)\n",
    "plt.title('Confusion matrix for test set predictions', fontsize=14)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "print('Accuracy: {:.2f}%'.format(acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
