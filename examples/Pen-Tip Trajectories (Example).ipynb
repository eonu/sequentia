{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducible randomness\n",
    "np.random.seed(72)\n",
    "rng = np.random.RandomState(72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/character-trajectories/mixoutALL_shifted.mat'\n",
    "\n",
    "try:\n",
    "    path = os.path.join(os.getcwd(), 'temp.mat')\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    raise\n",
    "else:\n",
    "    with open(path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "        data = loadmat(path)\n",
    "finally:\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trajectories\n",
    "X = [x.T for x in data['mixout'][0]]\n",
    "print('Number of trajectories: {}'.format(len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only lowercase characters with a single pen-down segment were considered in this dataset. In total, there were 20 of these characters as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the set of unique labels and report the number of labels\n",
    "labels = [label[0] for label in data['consts'][0][0][3][0]]\n",
    "n_labels = len(labels)\n",
    "print('Labels: {}'.format(str(labels)))\n",
    "print('Number of labels: {}'.format(n_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distribution of observation sequence lengths\n",
    "plt.title('Histogram of observation sequence lengths')\n",
    "plt.xlabel('Number of time frames')\n",
    "plt.ylabel('Count')\n",
    "plt.hist([len(x) for x in X], bins=n_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample rate of each trajectory recording was 200hz–meaning that in every second, 200 pen-tip trajectories were recorded!\n",
    "\n",
    "As seen in the histogram above, most characters can be drawn in less than 200 frames, or in less than one second.\n",
    "\n",
    "Although keeping all of these frames/data-points might result in a more accurate classifier, it also significantly increases the time required for training or prediction. This is especially the case for $k$NN, since it is a non-parametric classifier that requires going through each training example during prediction time.\n",
    "\n",
    "---\n",
    "\n",
    "There are two features offered by Sequentia that can help to reduce this issue:\n",
    "\n",
    "- Downsampling (summarizing each trajectory in a fewer number of frames) through two different methods:\n",
    "  - **Decimation**: Only keeping the observation at every every $n$th time frame.\n",
    "  - **Averaging**: Averaging every group of $n$ observations to form a singe observation.\n",
    "- Using a faster, restricted distance measure that can handle sequences of different length (see [FastDTW](https://pdfs.semanticscholar.org/05a2/0cde15e172fc82f32774dd0cf4fe5827cad2.pdf))\n",
    "\n",
    "The `DTWKNN` class always uses the FastDTW algorithm to calculate distances. Downsampling is offered as one of the preprocessing methods in Sequentia, and is used as follows (see the _Preprocessing_ notebook for more information):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequentia.preprocessing import downsample\n",
    "\n",
    "# Pick an example trajectory for visualization\n",
    "x = X[0]\n",
    "# Downsample the example trajectory, using a downsample factor of n=10\n",
    "x_down = downsample(x, n=10, method='average')\n",
    "\n",
    "# Create the plot to visualize the downsampling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "ax1.plot(x)\n",
    "ax1.set_title('Original velocity and force pen-tip trajectory sample')\n",
    "ax1.legend(labels=['$x$ velocity', '$y$ velocity', 'pen-tip force'])\n",
    "ax2.plot(x_down)\n",
    "ax2.set_title('Downsampled ($n=10$) velocity and force pen-tip trajectory sample')\n",
    "ax2.legend(labels=['$x$ velocity', '$y$ velocity', 'pen-tip force'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the entire dataset\n",
    "X = downsample(X, n=10, method='average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels\n",
    "y = [labels[idx - 1] for idx in data['consts'][0][0][4][0]]\n",
    "\n",
    "# Plot a histogram of the labels for each class\n",
    "plt.title('Histogram of the dataset label counts')\n",
    "plt.xlabel('Label (character)')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(y, bins=n_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split the dataset into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rng, shuffle=True)\n",
    "print('Training set size: {}'.format(len(X_train)))\n",
    "print('Test set size: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for displaying results (accuracy and confusion matrix)\n",
    "def show_results(acc, cm, dataset):\n",
    "    df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.title('Confusion matrix for {} set predictions'.format(dataset), fontsize=14)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    # Fix for matplotlib bug that cuts off top/bottom of seaborn visualizations\n",
    "    b, t = plt.ylim()\n",
    "    plt.ylim(b + 0.5, t - 0.5)\n",
    "    plt.show()\n",
    "    print('Accuracy: {:.2f}%'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping $k$-NN\n",
    "\n",
    "TODO\n",
    "\n",
    "---\n",
    "\n",
    "Importing, creating and fitting the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequentia.classifiers import DTWKNN\n",
    "\n",
    "# Create and fit a DTWKNN classifier using the single nearest neighbor and a radius of 1\n",
    "# NOTE: The radius parameter is a parameter that constrains the FastDTW algorithm.\n",
    "clf = DTWKNN(k=1, radius=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict single or multiple examples, we can use the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the first test example\n",
    "clf.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the first 5 test examples\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the model's accuracy and confusion matrix on some data, we can use the `evaluate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc, cm = clf.evaluate(X_test, y_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(acc, cm, dataset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Dynamic Time Warping $k$-NN classification often works with near perfect performance, but suffers due to the fact that $k$-NN is a non-parametric machine learning algorithm. \n",
    "\n",
    "This means that we have to look through every training example when we make a single prediction. Even with FastDTW and downsampling, the example classification on the test set consisting of 858 examples took over two and a half hours!\n",
    "\n",
    "---\n",
    "\n",
    "As a result, parametric methods such as Hidden Markov Models are often more feasible to use–but also generally perform worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Hidden Markov Models\n",
    "\n",
    "TODO\n",
    "\n",
    "---\n",
    "\n",
    "Creating the individual HMMs and fitting each one on the training examples corresponding to the label (character) that it represents.\n",
    "\n",
    "**Note**: Here we naively set the number of states for all HMMs to 10. In reality, you will probably want to have different numbers of states for HMMs that represent more complex or more simple classes (characters in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequentia.classifiers import HMM, HMMClassifier\n",
    "\n",
    "hmms = []\n",
    "for label in tqdm(labels, desc=\"Training HMMs\"):\n",
    "    hmm = HMM(label=label, n_states=10, random_state=rng, n_jobs=-1)\n",
    "    hmm.set_random_initial()\n",
    "    hmm.set_random_transitions()\n",
    "    hmm.fit([X_train[i] for i, y_i in enumerate(y_train) if y_i == label])\n",
    "    hmms.append(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HMMClassifier()\n",
    "clf.fit(hmms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, cm = clf.evaluate(X_test, y_test, labels=labels)\n",
    "show_results(acc, cm, dataset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, some HMMs might require a higher or lower number of states depending on how complex or simple the representing class is. In this case, the HMM representing the `w` trajectory should possibly have more states, as `w` is generally a more 'complex' character to draw compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
