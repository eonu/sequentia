# Copyright (c) 2019 Sequentia Developers.
# Distributed under the terms of the MIT License (see the LICENSE file).
# SPDX-License-Identifier: MIT
# This source code is part of the Sequentia project (https://github.com/eonu/sequentia).

"""A hidden Markov model with multivariate Gaussian mixture emissions."""

from __future__ import annotations

import typing as t

import hmmlearn.hmm
import numpy as np
import pydantic as pyd

from sequentia import enums
from sequentia._internal import _validation
from sequentia._internal._typing import FloatArray
from sequentia.models.hmm.variants.base import BaseHMM

__all__ = ["GaussianMixtureHMM"]


class GaussianMixtureHMM(BaseHMM):
    """A hidden Markov model with multivariate Gaussian mixture emissions.

    Examples
    --------
    Using a :class:`.GaussianMixtureHMM` to learn how to recognize spoken
    samples of the digit 3.

    See :func:`.load_digits` for more information on the sample dataset
    used in this example.

    ::

        import numpy as np
        from sequentia.datasets import load_digits
        from sequentia.models.hmm import GaussianMixtureHMM

        # Seed for reproducible pseudo-randomness
        random_state = np.random.RandomState(1)

        # Fetch MFCCs of spoken samples for the digit 3
        data = load_digits(digits=[3])
        train_data, test_data = data.split(test_size=0.2, random_state=random_state)

        # Create and train a GaussianMixtureHMM to recognize the digit 3
        model = GaussianMixtureHMM(random_state=random_state)
        X_train, lengths_train = train_data.X_lengths
        model.fit(train_data.X, lengths=train_data.lengths)

        # Calculate the log-likelihood of the first test sample being generated by this model
        x, y = test_data[0]
        model.score(x)
    """  # noqa: E501

    _DTYPE: type = np.float64
    _UNIVARIATE: bool = False

    @pyd.validate_call(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self: pyd.SkipValidation,
        *,
        n_states: pyd.PositiveInt = 5,
        n_components: pyd.PositiveInt = 3,
        covariance: enums.CovarianceMode = enums.CovarianceMode.SPHERICAL,
        topology: enums.TopologyMode | None = enums.TopologyMode.LEFT_RIGHT,
        random_state: pyd.NonNegativeInt | np.random.RandomState | None = None,
        hmmlearn_kwargs: dict[str, t.Any] | None = None,
    ) -> pyd.SkipValidation:
        """Initializes the :class:`.GaussianMixtureHMM`.

        Parameters
        ----------
        self: GaussianMixtureHMM

        n_states:
            Number of states in the Markov chain.

        n_components:
            Number of Gaussian components in the mixture emission
            distribution for each state.

        covariance:
            Type of covariance matrix in the mixture emission distribution
            for each state - see :ref:`covariance_types`.

        topology:
            Transition topology of the Markov chain — see :ref:`topologies`.

            If ``None``, behaves the same as ``'ergodic'`` but with
            `hmmlearn <https://hmmlearn.readthedocs.io/en/latest/>`__
            initialization.

        random_state:
            Seed or :class:`numpy:numpy.random.RandomState` object for
            reproducible pseudo-randomness.

        hmmlearn_kwargs:
            Additional key-word arguments provided to the
            `hmmlearn <https://hmmlearn.readthedocs.io/en/latest/>`__ HMM
            constructor.

        Returns
        -------
        GaussianMixtureHMM
        """
        super().__init__(
            n_states=n_states,
            topology=topology,
            random_state=random_state,
            hmmlearn_kwargs=hmmlearn_kwargs,
        )
        self.n_components: int = n_components
        """Number of Gaussian components in the emission model mixture
        distribution for each state."""

        self.covariance: enums.CovarianceMode = covariance
        """Type of covariance matrix in the emission model mixture
        distribution for each state."""

    @property
    @_validation.requires_fit
    def n_params(self: GaussianMixtureHMM) -> int:
        """Number of trainable parameters — requires :func:`fit`."""
        n_params = super().n_params()
        if "m" not in self._skip_params:
            n_params += self.model.means_.size
        if "c" not in self._skip_params:
            n_params += self.model.covars_.size
        if "w" not in self._skip_params:
            n_params += self.model.weights_.size
        return n_params

    def set_state_means(
        self: GaussianMixtureHMM,
        means: FloatArray,
        /,
    ) -> None:
        """Set the mean vectors of the state emission distributions.

        If this method is **not** called, mean vectors will be
        initialized by
        `hmmlearn <https://hmmlearn.readthedocs.io/en/latest/>`__.

        Parameters
        ----------
        self: GaussianMixtureHMM

        means:
            Array of mean values.

        Notes
        -----
        If used, this method should normally be called before :func:`fit`.
        """
        self._means = np.array(means, dtype=np.float64)
        self._skip_init_params |= set("m")

    def set_state_covars(
        self: GaussianMixtureHMM,
        covars: FloatArray,
        /,
    ) -> None:
        """Set the covariance matrices of the state emission distributions.

        If this method is **not** called, covariance matrices will be
        initialized by
        `hmmlearn <https://hmmlearn.readthedocs.io/en/latest/>`__.

        Parameters
        ----------
        self: GaussianMixtureHMM

        covars:
            Array of covariance values.

        Notes
        -----
        If used, this method should normally be called before :func:`fit`.
        """
        self._covars = np.array(covars, dtype=np.float64)
        self._skip_init_params |= set("c")

    def set_state_weights(
        self: GaussianMixtureHMM,
        weights: FloatArray,
        /,
    ) -> None:
        """Set the component mixture weights of the state emission
        distributions.

        If this method is **not** called, component mixture weights will be
        initialized by
        `hmmlearn <https://hmmlearn.readthedocs.io/en/latest/>`__.

        Parameters
        ----------
        self: GaussianMixtureHMM

        weights:
            Array of component mixture weights.

        Notes
        -----
        If used, this method should normally be called before :func:`fit`.
        """
        self._weights = np.array(weights, dtype=np.float64)
        self._skip_init_params |= set("w")

    def freeze(
        self: GaussianMixtureHMM,
        params: str | None = None,
        /,
    ) -> None:
        """Freeze the trainable parameters of the HMM,
        preventing them from be updated during the Baum—Welch algorithm.

        Parameters
        ----------
        self: GaussianMixtureHMM

        params:
            A string specifying which parameters to freeze. Can contain a
            combination of:

            - ``'s'`` for initial state probabilities,
            - ``'t'`` for transition probabilities,
            - ``'m'`` for emission distribution means,
            - ``'c'`` for emission distribution covariances,
            - ``'w'`` for emission distribution mixture weights.

        See Also
        --------
        unfreeze:
            Unfreeze the trainable parameters of the HMM,
            allowing them to be updated during the Baum—Welch algorithm.
        """
        super().freeze(params)

    def unfreeze(
        self: GaussianMixtureHMM,
        params: str | None = None,
        /,
    ) -> None:
        """Unfreeze the trainable parameters of the HMM,
        allowing them to be updated during the Baum—Welch algorithm.

        Parameters
        ----------
        self: GaussianMixtureHMM

        params:
            A string specifying which parameters to unfreeze. Can contain
            a combination of:

            - ``'s'`` for initial state probabilities,
            - ``'t'`` for transition probabilities,
            - ``'m'`` for emission distribution means,
            - ``'c'`` for emission distribution covariances,
            - ``'w'`` for emission distribution mixture weights.

        See Also
        --------
        freeze:
            Freeze the trainable parameters of the HMM,
            preventing them from be updated during the Baum—Welch algorithm.
        """
        super().unfreeze(params)

    def _init_hmm(
        self: GaussianMixtureHMM,
        **kwargs: t.Any,
    ) -> hmmlearn.hmm.GMMHMM:
        """Initialize the hmmlearn model."""
        return hmmlearn.hmm.GMMHMM(
            n_components=self.n_states,
            n_mix=self.n_components,
            covariance_type=self.covariance.value,
            random_state=self.random_state_,
            **kwargs,
        )

    @staticmethod
    def _hmmlearn_kwargs_defaults() -> dict[str, t.Any]:
        """Default values for hmmlearn key-word arguments."""
        return {"init_params": "stmcw", "params": "stmcw"}

    @staticmethod
    def _unsettable_hmmlearn_kwargs() -> list[str]:
        """Arguments that should not be provided in `hmmlearn_kwargs` in
        :func:`__init__`.
        """
        return [
            *BaseHMM._unsettable_hmmlearn_kwargs(),  # noqa: SLF001
            "n_components",
            "n_mix",
            "covariance_type",
        ]

    @staticmethod
    def _hmmlearn_params() -> list[str]:
        """Names of trainable hmmlearn parameters."""
        return [
            *BaseHMM._hmmlearn_params(),  # noqa: SLF001
            "means",
            "covars",
            "weights",
        ]
